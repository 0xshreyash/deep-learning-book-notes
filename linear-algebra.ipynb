{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hide_input": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Linear Algebra\n",
    "\n",
    "Linear Algebra is a branch of mathematics that is essential for understanding and working with many machine learning algorithms, especially deep learning algorithms. This chapter focuses on the topics within Linear Algebra that are relevant to machine learning and does not try to act as an exhaustive source of information of on the topic.\n",
    "\n",
    "## 2.1 Scalars, Vectors, Matrics and Tensors\n",
    "\n",
    "The principal mathemtical objects involved in linear algebra are:\n",
    "\n",
    "1. **Scalars** - A scalar is a single number that can take up many different values (real values, integers etc)\n",
    "\n",
    "2. **Vectors** - A vector is an array of numbers. An elemennt in the\n",
    "array is identified using its index in the ordering of the array. We can think \n",
    "of vectors as identifying points in space, each element in the\n",
    "vector giving the coordinates along a different axis. We say that a vector \n",
    "$x \\in \\mathbb{R}^{n}$ if $x$ has n elements.\n",
    "\n",
    "TODO: Add picture of a vector\n",
    "\n",
    "3. **Matrices** - A matrix is a 2-D array of numbers, so each\n",
    "element is identified by two indices instead of just one. In general for a matrix\n",
    "$A$ we say that $A \\in \\mathbb{R}^{m \\times n}$. A row of a matrix is identified\n",
    "by $A_{i, :}$ and similarly a columb by $A_{:, j}$ \n",
    "\n",
    "TODO: Add picture of a vector\n",
    "\n",
    "4. **Tensors** - An array of numbers on a regular gird with a variable number of axes is known as a tensor. \n",
    "A tensor \"A\" is denoted by __A__. In general we tend to use tensors with 3 axes such that __A__ $\\in \\mathbb{R}^{m\\times n \\times k}$ and an element of this tesnor is dentoed using three coordinates.\n",
    "\n",
    "Some important operations on matrices are:\n",
    "\n",
    "The __Transpose__ of a matrix which is the mirror image of the matrix across a\n",
    "diagonal line called the _main diagonal_, starting from the upper-left\n",
    "corner and going down and to the right. It essentially puts the\n",
    "element at position *(i, j)* of a matrix to position\n",
    "*(j, i)* of the transpose. __Note__: the transpose of the transpose of a\n",
    "matrix is the matrix itself.\n",
    "\n",
    "TODO: Add picture of transpose.\n",
    "\n",
    "The __Addition__ of two matrices, _A + B_ corresponds to the addition of two\n",
    "matrices. This is only possible as long as the two matrices have\n",
    "the same shape. This operation corresponds to adding elements\n",
    "at position *(i, j)* of _A and B_ and assigning their sum to the\n",
    "*(i, j)*-th element of the resultant matrix, let's say _C_.\n",
    "\n",
    "__Adding or multiplying a scalar__ to a matrix corresponds to performing\n",
    "the addition/multiplication by the scalar to each element of the matrix.\n",
    "\n",
    "__Broadcasting__ is a less conventional method used in deep learning\n",
    "wherein we allow the addition of a matrix to a vector yielding another\n",
    "matrix. Lets say we perform $C = A + b$ where b is a column vector (a\n",
    "vector with just one column). This corresponds to adding to each element\n",
    "in $A_{i, :}$ the value at $b_{i}$. In a similar vain, if we were to\n",
    "add a row vector to $A$ it would result in the addition of $b_{j}$\n",
    "to every element of column $j$ of $A$.\n",
    "\n",
    "TODO: Add image of broadcasting here\n",
    "\n",
    "\n",
    "## 2.2 Multiplying Matrices and Vectors\n",
    "\n",
    "__Matrix multiplication__ is the product of matrices $A$ and $B$ resulting in a matrix $C$.\n",
    "In order for this product to be defined the number of columns in $A$ need to be the same as\n",
    "the number of rows in $B$. If $A$ is of shape $m \\times n$ and $B$ is of shape \n",
    "$n \\times p$ then $C$ will be of shape $m \\times p$. The notation for\n",
    "matrix product is simply $C = AB$ where the product operation\n",
    "is defined by $C_{i, j} = \\sum_{k} A_{i, k} B_{k, j} $.\n",
    "\n",
    "The __Hadamard product__ of two matrices is denoted by A âŠ™ B and is the element-wise product\n",
    "of two matrices. This corresponds to multiplying element _i, j_ of $A$ with\n",
    "element _i, j_ of $B$.\n",
    "\n",
    "The __dot product__ between two vectors $x$ and $y$ of the same\n",
    "dimensionality is the matrix product $x^{T}y$. We can think of the matrix\n",
    "product $C = AB$ as computing $C_{i, j}$ the dot product between row i of A\n",
    "and column j of B.\n",
    "\n",
    "Matrix products have the following mathematical properties:\n",
    "\n",
    "* Distributive Nature - $A(B + C) = AB + AC$\n",
    "* Associativity - $A(BC) = (AB)C$\n",
    "\n",
    "Matrix multiplication is not commutative but the dot product between two\n",
    "vectors is commutative since it results in a scalar.\n",
    "\n",
    "The transpose of a matrix product has the simple form:\n",
    "\n",
    "$(AB)^{T} = B^{T}A^{T}$\n",
    "\n",
    "A system of linear equations can be denoted by: $Ax = b$, where $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$ and $x \\in \\mathbb{R}^n$ is the vector of unknows we would like to solve for. To solve this system of linear equations linear algebra provides us with the matrix inversion operation.\n",
    "\n",
    "\n",
    "## 2.3 Identity and Inverse Matrices\n",
    "\n",
    "The __identity matrix__ is a matrix that does not change any vector when we multiply\n",
    "that vector by that matrix. We denote the idenity matrix that preserves n-dimensional\n",
    "vectors as $I_{n}$j. The entries along the main diagonal of an identity matrix \n",
    "are 1 whilst the rest of the entries are 0.\n",
    "\n",
    "TODO: add picture of the identity matrix.\n",
    "\n",
    "The __matrix inverse__ of A is denoted as $A^{-1}$ and it is defined as:\n",
    "\n",
    "$$ A^{-1} A = I_{n} $$ \n",
    "\n",
    "Multiplying both sides of $Ax = b$ by $A^{-1}$ on the left gives us x = $A^{-1}b $. Of course only this process only works if matrix $A$ has an inverse.\n",
    "\n",
    "## 2.4 Linear Dependence and Span\n",
    "\n",
    "For $A^{-1}$ to exist equation the equation $Ax = b$ must have exactly one solution for every value of\n",
    "$b$. It is also possible for the system of equations to have no solutions or infinitely\n",
    "many solutions. It is not possible to have more than one and less than an infinite number of solutions since if $x$ and $y$ are solutions for $a$ particular $b$ then:\n",
    "\n",
    "$$ z = \\alpha x + (1 - \\alpha) y $$\n",
    "\n",
    "is also a solution. To analyze how many solution $Ax = b$ has we can think of columns of $A$ as specifying different directions we can travel in from the __origin__, then count the number of ways there are of reaching $b$. Let's look at this in a bit more detail:\n",
    "\n",
    "\\begin{equation}\n",
    "Ax  = \n",
    "\\begin{bmatrix} \n",
    "A_{1, 1}x_{1} + ... + A_{1, n}x_{n} \\\\\n",
    "A_{2, 1}x_{1} + ... + A_{2, n}x_{n} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "A_{m, 1}x_{1} + ... + A_{m, n}x_{n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Ax = [A_{1, 1} + ... + A_{m, 1}]x_{1} + ... + [A_{1, n}  + ... + A_{m, n}]x_{n}\n",
    "\\end{equation}\n",
    "\n",
    "This shows us that each $x_{i}$ specifies how far we should move in the direction of the column $A_{:, i}$ of $A$. Such an operation is called a __linear combination__ where given a set of vectors ${v^{(i)},....,v^{(n)}}$ we define $\\sum_{i} c_{i}v^{(i)}$.\n",
    "\n",
    "The __span__ of a set of vectors is the set of all points obtainable by linear combination of\n",
    "all the original vectors. Testing whether $Ax = b$ comes down to checking whether $b$ is in\n",
    "the span of the columns of $A$. This particular span is known as the column space, or the\n",
    "range of $A$. To be able to cover $R^{m}$ we need at least m vectors in A. This is a necessary\n",
    "condition but not a sufficient one. In particular we are required to have m vectors that are\n",
    "linearly independent of each other to be able to represent $R^{m}$. A set of vectors is linearly\n",
    "independent if no vector in the set is a linear combination of the other vectors. A set of m\n",
    "linearly independent vectors will guarantee that we cover all $b \\in \\mathbb{R}^{m}$.\n",
    "\n",
    "But to have an inverse a matrix must have at most one solution for each value of b.\n",
    "To do this we must make sure that that the matrix must have at most m columns.\n",
    "\n",
    "This means that a matrix needs to be square and the columns need to be linearly\n",
    "independent for an inverse to  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "## Suggested Resources\n",
    "\n",
    "[The Matrix Cookbook](https://www.ics.uci.edu/~welling/teaching\\/KernelsICS273B/MatrixCookBook.pdf)\n",
    "\n",
    "[Shilov](https://cosmathclub.files.wordpress.com/2014/10/georgi-shilov-linear-algebra4.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
