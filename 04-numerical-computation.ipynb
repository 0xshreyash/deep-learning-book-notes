{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T18:47:33.664764Z",
     "start_time": "2019-07-07T18:47:33.656967Z"
    }
   },
   "source": [
    "# Numerical Computation\n",
    "\n",
    "Machine Learning involves numerical computations that are used to solve mathematical problems via an iterative process that updates the estimates of the solution until some convergence criterion is reached. This is contrary to analytically determining a formula to provide a symbol expression for the correct solution which is a common approach in both engineering and mathematics. Common numerical operations used in ML are:\n",
    "\n",
    "1. Optimization\n",
    "2. Solving a system of linear equations\n",
    "\n",
    "## 4.1 Overflow and Underflow\n",
    "\n",
    "On a computer, we use a finite number of bit patterns (due to a finite amount of available memory) to represent infinitely many real numbers. This causes us to make approximations that can lead to rounding errors and can even cause a theoretically proven algorithm to fail in practice.\n",
    "\n",
    "__Underflow__ is one such form of rounding error and occurs when a number near zero gets rounded off to zero. This can cause operations such as \"divide by\" or \"log\" to fail since both operations are undefined for 0.\n",
    "\n",
    "__Overflow__ on the other hand occurs when numbers with large magnitudes are approximated as $\\inf$ or $-\\inf$. \n",
    "\n",
    "The __softmax function__ $softmax(\\pmb{x})_i = \\dfrac{\\exp(x_i)}{\\sum_{j = 1}^{n}\\exp(x_j)}$ for example, must be stabilized against underflow and overflow. Let's take a look at this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T21:39:42.472134Z",
     "start_time": "2019-07-07T21:39:42.463719Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Overflow error for an array with all values = 4294967296.0\n",
      "Divison By Zero error for an array with all values = -4294967296.0\n",
      "0.1\n",
      "0.1\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def softmax(x, i):\n",
    "    return math.exp(x[i]) / sum(map(lambda v: math.exp(v), x))\n",
    "\n",
    "xs = [[1] * 10, [math.pow(2, 32)] * 10, [-math.pow(2, 32)] * 10]\n",
    "\n",
    "# softmax without exponent\n",
    "for x in xs:\n",
    "    try: \n",
    "        print(softmax(x, 1))\n",
    "    except OverflowError:\n",
    "        print(f'Overflow error for an array with all values = {x[0]}')\n",
    "    except ZeroDivisionError:\n",
    "        print(f'Divison By Zero error for an array with all values = {x[0]}')\n",
    "\n",
    "# Normalize the values in x before taking the exponent\n",
    "for x in xs:\n",
    "    try:\n",
    "        m = max(x)\n",
    "        zs = list(map(lambda v: v - m, x))\n",
    "        print(softmax(zs, 1))\n",
    "    except OverflowError:\n",
    "        print(f'Overflow error for an array with all values = {x[0]}')\n",
    "    except ZeroDivisonError:\n",
    "        print(f'Divison By Zero error for an array with all values = {x[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see from the the example above that when $\\forall i, x_i = c$ and x has length n then we expect the value of the softmax to be $\\dfrac{1}{n}$. But that is not what happens when c is very large or small. To maneuver against this what we do is modify each element in the array, xs using the following operation $\\forall i, z_i = x_i - max_j x_j$ and then use the resulting zs to calculate softmax. This maneuvering does not alter the expected output and also allows us to tackle the issues of overflow and division by zero.\n",
    "\n",
    "However, if our numerator $x_i$ is a large negative number and all the other numbers in the list of values are positive. The numerator of softmax might get round off to zero resulting in underflow. Moreover, if we take $\\log(softmax(\\pmb{x})$ then we would end up with $-\\inf$. But to counteract this we make use of log rules in evaluating $\\log(softmax(x)_i)$. We thus present a completely stabilized version of $\\log(softmax(x)_i)$ below:\n",
    "\n",
    "$$\n",
    "\\log(softmax(x)_i) = \\log\\left(\\dfrac{\\exp(x_i)}{\\sum_{j = 1}^{n}\\exp(x_j)}\\right) \\\\\n",
    "= \\log\\left(\\dfrac{\\exp(x_i - b)\\exp(b)}{\\sum_{j = 1}^{n}\\exp(x_j - b)\\exp(b)}\\right) \\\\ \n",
    "= \\log\\left(\\exp(x_i - b)) - \\log(\\sum_j \\exp(x_j - b)\\right) \\\\\n",
    "= x_i - \\log\\left(\\sum_j \\exp(x_j - b)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Poor Conditioning\n",
    "\n",
    "Conditioning refers to how rapidly a function changes with respect to small changes in its input. When a function changes rapidly with small changes to its input it can be problematic for scientific computations because of rounding errors. \n",
    "\n",
    "Let's consider $f(\\pmb x) = A^{-1} \\pmb{x}$ assuming $A$ has an eigenvalue decomposition. The __condition number__ for $A$ is:\n",
    "\n",
    "$$\n",
    "\\max_{i, j} \\left|\\dfrac{\\lambda_i}{\\lambda_j}\\right|\n",
    "$$\n",
    "\n",
    "which is the ratio of the largest and smallest eigenvalues. When this number is large matrix inversion is particularly sensitive to error in inputs. This sensitivity is intrinsic to the matrix itself not the result of rounding error during matrix inversion. Poorly conditioned matrices amplify pre-existing errors when we multiply by the true matrix inverse.\n",
    "\n",
    "\n",
    "## 4.3 Gradient-Based Optimization\n",
    "\n",
    "Most deep learning algorithms involve optimization of some sort. Optimization refers to the task of minimizing or maximizing some function $f(x)$ by changing $x$. Here $f$ is the __objective function__ or __criterion__. In deep learning we frame problems as minimizing $f(x)$. During minimization $f$ might also be called the __cost function__, __loss function__ or __error function__. $x^*$ is the value that minimizes the function $f$, so $x^* = arg min f(x)$.\n",
    "\n",
    "Let's say $y = f(x)$. The __derivative__ of this function is denoted by $f'(x)$ or as $\\dfrac{dy}{dx}$. The derivative gives the slow of $f$ at $x$. It specifies how to scale a small change in the input to obtain a corresponding change in the output: $f(x + \\epsilon) \\approx f(x) + \\epsilon f'(x)$.\n",
    "\n",
    "The derivative is used for minimizing a function because it tells us how to change $x$ to make a small improvement in $y$. We can decrease $f(x)$ by moving $x$ in small steps in the direction with the opposite sign to the derivative. This technique is called __gradient descent__.\n",
    "\n",
    "TODO: Add picture 4.1 from book here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $f'(x) = 0$ the derivative gives us no information about the direction we should move in. These points where the derivative is 0 are known as __critical points__ or __stationary points__. We can classify them as:\n",
    "\n",
    "1. Local minimum - a point where $f(x)$ is lower than all neighboring points. \n",
    "2. Local maximum - a points where $f(x)$ is higher than all neighboring points.\n",
    "3. Saddle Points - a point where $f(x)$ is neither a maximum or a minimum.\n",
    "\n",
    "TODO: Add picture 4.2 here.\n",
    "\n",
    "A point that obtains the lowest value of $f(x)$ across all possible $x$ is called the __global minimum__. A local minimum does not have to be a global minimum. In deep learning we usually settle for finding a value that is very low but not necessarily  minimal in any sense due to the complex nature of functions being minimized.\n",
    "\n",
    "We often minimize function that are defined as $f: \\mathbb{R}^n \\implies \\mathbb{R}$. For minimization to make sense the output must be a scalar. In case, of multiple inputs we must take __partial derivatives__. The partial derivative $\\dfrac{\\partial f(\\pmb{x})}{\\partial x_i}$ measures what direction we should move $x_i$ in to be able to increase $f$ all other variables staying the same. The gradient of a function is denoted as $\\bigtriangledown_{\\pmb{x}}f(\\pmb{x})$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
