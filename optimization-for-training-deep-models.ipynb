{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Optimization for Training Deep Models\n",
    "\n",
    "Neural Network training is the most difficult optmization problem involved in deep learning. Owing to this diffculty of training neural networks a specialized of optimization techniques have been developed for solving it. We are concerned with finding parameters $\\theta$ that greatly reduce a cost function $J(\\theta)$ (which includes a performance measure on the entier training set as well as additional regularization terms).\n",
    "\n",
    "## 8.1 How Learning Differs from Pure Optimization\n",
    "\n",
    "In machine learning we care about optimizing a performance measure $P$ that is defined in terms of a test set and mat also be intractable. Therefore, we reduce a different cost function $J(\\theta)$ in hope that doing so will improve $P$. This is in contrast to pure optimization where minimizing $J$ is the goal in and of itself. Optimization algorithms for training deep models also include some specialization on the specific structure of machine learning objective f unctions. The cost function can be written as:\n",
    "\n",
    "$$J(\\theta) =  \\mathbb{E}_{(x, y)\\sim\\hat{p}_{data}}L(f(x; \\theta), y)$$\n",
    "\n",
    "where $L$ is the per example loss function, $f(x; \\theta)$ is the predicted output when the input is $x$, $\\hat{p}_{data}$ is the emperical distribution and $y$ is the target output in the supervised learning case. In general we would prefer to minimize the objective function where the expectation is taken across the _data-generating distribution_ $p_{data}$ rather than the _emperical distribution_ defined over the finite training set.\n",
    "\n",
    "### 8.1.1 Emperical Risk Minimization\n",
    "\n",
    "The goal of machine learning is to minimize the expected generalization error over the data generating distribution (this quantity is known as __risk__). If $p_{data}$ was know known then risk minimization would be an optimization problem. When we only have $\\hat{p}_{data}$ we have a machine learning problem on our hands. We can convert a machine learning problem back to an optimization problem by optimizing over the emperical distributuion maximizing over the __emperical risk__ via a process called __emperical risk minimization__. But emperical risk minimization suffers from the following issues:\n",
    "\n",
    "* It is prone to overfitting the training set.\n",
    "\n",
    "* It might not be feasible in certain cases (such as when the derivatives of a loss function e.g. 0-1 loss aren't meaningful).\n",
    "\n",
    "### 8.1.2 Surrogate Loss Function and Early Stopping\n",
    "\n",
    "Due to the fact that the loss function we actually care about (say, classification error on the test set) is sometimes difficult to optimize we optimize what is called a __surrogate loss function__ instead. The negative log-likelihood is typically used as a surrogate for 0-1 loss. The negative log-likelihood allows the model to estimate the conditioanl probablity of the classes given the input and if the model can do that well then it can pick the classes that yield the least classification error in expectation. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
